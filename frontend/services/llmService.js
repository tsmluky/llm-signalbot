// frontend/services/llmService.js
import { API_ANALYZE } from "../constants";

export async function getLLMResponse(prompt, token, mode = "lite") {
  try {
    const response = await fetch(API_ANALYZE, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ token, message: prompt, mode }),
    });

    const data = await response.json();
    console.log("[üîç DATA RAW]:", data);

    if (!response.ok || data.status !== "ok") {
      const msg = data.message || "Fallo desconocido.";
      throw new Error(`‚ùå No se pudo generar el an√°lisis. ${msg}`);
    }

    const analysis = data.analysis;
    if (!analysis || typeof analysis !== "string" || analysis.trim() === "") {
      console.warn("‚ö†Ô∏è El campo 'analysis' est√° vac√≠o o malformado.");
      return {
        status: "error",
        analysis: "‚ùå El modelo no devolvi√≥ contenido √∫til.",
        token,
        mode,
        prompt,
        timestamp: new Date().toISOString(),
      };
    }

    return {
      status: "ok",
      analysis,
      token: data.token || token,
      mode: data.mode || mode,
      prompt: data.prompt || prompt,
      timestamp: data.timestamp || new Date().toISOString(),
    };
  } catch (error) {
    console.error("‚ùå Error en getLLMResponse:", error);
    return {
      status: "error",
      analysis: "‚ùå Error inesperado al generar el an√°lisis.",
      token,
      mode,
      prompt,
      timestamp: new Date().toISOString(),
    };
  }
}
